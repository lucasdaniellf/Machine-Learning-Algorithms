# -*- coding: utf-8 -*-
"""LinearRegression_Boston_Household.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18RZk9IJPkgHvPBA8Vk-POLSIsDDCxIQC

## Machine Learning Algorithms without Scikit-Learn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

data = load_boston()

df_boston = data.data
df_boston.shape

df_target = data.target
df_target.shape

data.feature_names

df_table = pd.DataFrame(data=df_boston, columns=data.feature_names)
df_table.head()

df_table = df_table.join(pd.Series(np.transpose(df_target), name='PRICE'))
df_table.head()

pd.notna(df_table).all()

df_table.describe()

features_mean = (df_table.mean()[0:13]).to_numpy()
diff_max_min = ((df_table.max() - df_table.min())[0:13]).to_numpy()

scaled_df_boston = (df_boston - features_mean)/diff_max_min

X_train, X_test, y_train, y_test = train_test_split(scaled_df_boston, df_target, test_size=0.3, random_state = 0)

X0 = np.ones([X_train.shape[0], 1])
X_train_X0 = np.concatenate((X0, X_train), axis=1)
Theta = np.zeros([X_train_X0.shape[1]])

h_theta = np.sum(X_train_X0*Theta, axis=1)

h_theta = np.sum(X_train_X0*Theta, axis=1)
Cost = 1/(2*X_train_X0.shape[0])*np.sum(((h_theta - y_train)**2))
Cost
#Theta_temp = ((0.03)/(X_train_X0.shape[0]))*np.sum((h_theta - y_train)*(X_train_X0.transpose()), axis=1)
#h_theta = np.array([h_theta]).transpose()

i = 0
J_func = []
Theta = np.zeros([X_train_X0.shape[1]])
while i<10000 and Cost>8:
  h_theta = np.sum(X_train_X0*Theta, axis=1)
  Cost = 1/(2*X_train_X0.shape[0])*np.sum(((h_theta - y_train)**2))
  J_func.append(Cost)
  Theta_temp = ((0.05)/(X_train_X0.shape[0]))*np.sum((h_theta - y_train)*(X_train_X0.transpose()), axis=1)
  Theta = Theta - Theta_temp
  i = i+1
print(Cost, i)

plt.plot(range(0,len(J_func)), J_func);

plt.ylabel('Cost');
plt.xlabel('Iterations');
plt.grid()

X0_test = np.ones([X_test.shape[0], 1])
X_test_X0 = np.concatenate((X0_test, X_test), axis=1)

h_theta_train = np.sum(X_train_X0*Theta, axis=1)
rms = np.sqrt(mean_squared_error(y_train, h_theta_train))
rms

h_theta_test = np.sum(X_test_X0*Theta, axis=1)
rms = np.sqrt(mean_squared_error(y_test, h_theta_test))
rms

comparative = np.concatenate((np.array([y_test]).transpose(), np.array([h_theta_test]).transpose()), axis=1)

#Making the comparative matrix in crescent order (with respect with the predicted values)
comparative = comparative[comparative[:,1].argsort()]

plt.scatter(x=range(len(comparative[:,0])), y=comparative[:,0])
plt.plot(range(len(comparative[:,0])), comparative[:,1], color='red')

plt.grid()
plt.ylabel('Price');

plt.scatter(x=comparative[:,1], y=comparative[:,0])
plt.plot(comparative[:,1], comparative[:,1], color='red')

plt.grid()
plt.xlabel('Predicted');
plt.ylabel('Real Value');

"""## Linear Regression with SkLearn"""

from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True, normalize=True)
model.fit(X_train, y_train);

model.score(X_test, y_test)

comparative_sk = np.concatenate((np.array([y_test]).transpose(), np.array([model.predict(X_test)]).transpose()), axis=1)
comparative_sk = comparative_sk[comparative_sk[:,1].argsort()]

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey='row', figsize=(12,6))

ax1.scatter(x = comparative_sk[:,1], y = comparative_sk[:,0])
ax1.plot(comparative_sk[:,1], comparative_sk[:,1], color='red')
ax2.scatter(x = comparative[:,1], y = comparative[:,0])
ax2.plot(comparative[:,1], comparative[:,1], color='red')

ax1.grid();
ax2.grid();

ax1.set_ylabel('real')
ax1.set_xlabel('predicted')
ax2.set_xlabel('predicted')

ax1.set_title('SKlearn Model')
ax2.set_title('Gradient Descent Model');

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey='row', figsize=(12,6))

ax1.scatter(x = range(len(comparative_sk[:,0])), y = comparative_sk[:,0])
ax1.plot(range(len(comparative_sk[:,0])), comparative_sk[:,1], color='red')
ax2.scatter(x=range(len(comparative[:,0])), y=comparative[:,0])
ax2.plot(range(len(comparative[:,0])), comparative[:,1], color='red')

ax1.grid();
ax2.grid();

ax1.set_ylabel('real/predicted')
ax1.set_xlabel('index in matrix')
ax2.set_xlabel('index in matrix')

ax1.set_title('SKlearn Model')
ax2.set_title('Gradient Descent Model');

