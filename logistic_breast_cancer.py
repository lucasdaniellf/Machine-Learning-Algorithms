# -*- coding: utf-8 -*-
"""Logistic_Breast_Cancer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18hSIk0waVp_GfrVvbhURVeNo5ql_8b-V

## Logistic Regression without Sklearn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

data = load_breast_cancer()
data_features = data.data
data_target = data.target
data_table = pd.DataFrame(data=data_features, columns=data.feature_names)
data_table = data_table.join(pd.Series(data_target, name='Result'))

data_table.head()

data_features.shape

pd.notna(data_table).all()

data_table.describe()

data_mean = data_table.mean()[0:30].to_numpy()
data_diff = (data_table.max() - data_table.min())[0:30].to_numpy()

data_scaled = (data_features - data_mean)/data_diff

data_scaled.mean()

data_scaled.shape

X0 = np.ones([569,1])
data_scaled_X0 = np.concatenate((X0, data_scaled), axis=1) 
data_scaled_X0[0]

Theta = np.zeros([31])

X_train, X_test, y_train, y_test = train_test_split(data_scaled_X0, data_target, test_size=0.3, random_state=0)

z_theta = np.sum(X_train*Theta, axis=1)
z_theta.shape

def sigmoid(z):
  h_theta = 1/(1+(np.e**(-z)))
  return h_theta

#(0.01/len(Cost_func))*np.sum((h_theta - y_train)*X_train.transpose(), axis=1)

Theta = np.zeros([31])

#Just setting a high J number so the while loop can start

J=10
J_list = []
i = 0

while J>0.1 and i<10000:
  z_theta = np.sum(X_train*Theta, axis=1)
  h_theta = sigmoid(z_theta)
  Cost_func = -y_train*np.log(h_theta) - (1-y_train)*np.log(1 - h_theta)

  J = (1/len(Cost_func))*np.sum(Cost_func)
  J_list.append(J)

  Theta_temp = (0.09/len(Cost_func))*np.sum((h_theta - y_train)*X_train.transpose(), axis=1)
  Theta = Theta - Theta_temp
  i = i+1

print(J, i)

plt.plot(range(len(J_list)), J_list);
plt.grid();
plt.ylabel('Cost')
plt.xlabel('Iteration');

def verify(probability):
  prediction = []
  for i in probability:
    if i > 0.5:
      predict = 1
      prediction.append(predict)
    else:
      predict = 0
      prediction.append(predict)
  return prediction
  
z_test = np.sum(X_test*Theta, axis=1)
h_test = sigmoid(z_test)
h_test = h_test.tolist()

prediction = verify(h_test)

comparative = np.concatenate((np.array([prediction]).transpose(), np.array([y_test]).transpose()), axis=1)

True_Positives = comparative[(comparative[:,1] == 1) & (comparative[:,0] == 1)].shape[0]
False_Positives = comparative[(comparative[:,1] == 0) & (comparative[:,0] == 1)].shape[0]
True_Negatives = comparative[(comparative[:,1] == 0) & (comparative[:,0] == 0)].shape[0]
False_Negatives = comparative[(comparative[:,1] == 1) & (comparative[:,0] == 0)].shape[0]

Precision = True_Positives/(True_Positives+False_Positives)
Recall = True_Positives/(True_Positives+False_Negatives)

F1Score = 2*(Precision*Recall)/(Precision+Recall)
F1Score

"""## Logistic Regression with SKLearn"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

model.fit(X_train, y_train);

model.predict(X_test)

y_test

model.score(X_test, y_test)

comparative = np.concatenate((np.array([model.predict(X_test)]).transpose(), np.array([y_test]).transpose()), axis=1)
True_Positives_sk = comparative[(comparative[:,1] == 1) & (comparative[:,0] == 1)].shape[0]
False_Positives_sk = comparative[(comparative[:,1] == 0) & (comparative[:,0] == 1)].shape[0]
True_Negatives_sk = comparative[(comparative[:,1] == 0) & (comparative[:,0] == 0)].shape[0]
False_Negatives_sk = comparative[(comparative[:,1] == 1) & (comparative[:,0] == 0)].shape[0]

Precision_sk = True_Positives_sk/(True_Positives_sk+False_Positives_sk)
Recall_sk = True_Positives_sk/(True_Positives_sk+False_Negatives_sk)

F1Score_sk = 2*(Precision_sk*Recall_sk)/(Precision_sk+Recall_sk)
print(f' With sklearn (SVM): {round(F1Score_sk, 4)}, with Gradient Descent: {round(F1Score, 4)}')

True_Positives, True_Positives_sk

False_Positives, False_Positives_sk

True_Negatives, True_Negatives_sk

False_Negatives, False_Negatives_sk

