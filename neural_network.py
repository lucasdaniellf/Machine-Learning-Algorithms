# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_NQYt9MiHQOSHGFCCaB0U3X9cfs2CAG

## Neural Network - hand-draw number Recognition
"""

import numpy as np
import csv
from sklearn.model_selection import train_test_split

with open('Pixels.csv') as csv_file:
  csv_reader = csv.reader(csv_file)
  matrix = []
  for line in csv_reader:
    matrix.append(line)

X_1 = np.array(matrix).astype(np.float)
X_1[0, 125:140]

X_1.shape

with open('Target.csv') as csv_file:
  csv_reader = csv.reader(csv_file)
  target = []
  for line in csv_reader:
    target.append(line)

y = np.array(target).astype(int)
y = y.transpose()[0]
y.shape

num_labels = 10
hidden_layer = 25
input_layer = 400

#DO NOT TOUCH HERE (SO THE RAND GENERATED NUMBERS DO NOT CHANGE)
epsilon_init_1 = np.sqrt(6)/np.sqrt(hidden_layer + input_layer)
epsilon_init_2 = np.sqrt(6)/np.sqrt(hidden_layer + num_labels)

Theta_1 = np.random.rand(hidden_layer, input_layer+1)*epsilon_init_1*2 - epsilon_init_1
Theta_2 = np.random.rand(num_labels, hidden_layer+1)*epsilon_init_2*2 - epsilon_init_2
Theta_1_derivative = np.zeros(Theta_1.shape)
Theta_2_derivative = np.zeros(Theta_2.shape)

Theta_1.shape, Theta_2.shape, Theta_1_derivative.shape, Theta_2_derivative.shape

y_list = []
for i in range(len(y)):
  value = np.zeros(10)
  value[y[i]-1] = 1
  y_list.append(value)

y = np.array(y_list)

y.shape

#FEEDFORWARD
X0 = np.ones([X_1.shape[0],1])
X_1_X0 = np.concatenate((X0, X_1), axis=1) 
X_1_X0.shape, X_1_X0[1][0:5]

Z2 = np.dot(X_1_X0,Theta_1.transpose())
Z2.shape

X_2 = 1/(1+(np.e**(-Z2)))
X_2_X0 = np.concatenate((X0, X_2), axis=1) 
X_2_X0.shape

Z3 = np.dot(X_2_X0, Theta_2.transpose())
Z3.shape

y_predict = 1/(1+(np.e**(-Z3)))
y_predict.shape

Theta_2[:,1:].shape

# Regularization term
lambda_reg = 1
reg = lambda_reg/(2*X_1.shape[0])*(np.sum(np.sum((Theta_1[:,1:])**2, axis=1)) + 
                                   np.sum(np.sum((Theta_2[:,1:])**2, axis=1)))

(1/Cost_func.size)

Cost_func = np.sum(-y*np.log(y_predict) - (1-y)*np.log(1 - y_predict), axis=1)
J = (1/Cost_func.size)*np.sum(Cost_func) + reg
J, Cost_func.size

DELTA_1 = 0
DELTA_2 = 0

delta_3 = y - y_predict
delta_3.shape

np.concatenate((X0, X_2*(1-X_2)), axis=1)[0][0:5]

delta_2 = np.dot(delta_3, Theta_2)*np.concatenate((X0, X_2*(1-X_2)), axis=1)
delta_2 = delta_2[:, 1:]
delta_2.shape

DELTA_2 = np.dot(delta_3.transpose(), X_2_X0)
DELTA_1 = np.dot(delta_2.transpose(), X_1_X0)
DELTA_2.shape, DELTA_1.shape

np.concatenate((np.zeros([Theta_1.shape[0], 1]), Theta_1[:, 1:]), axis = 1).shape

Theta_1_derivative = (1/(X_1.shape[0]))*DELTA_1 + (lambda_reg/X_1.shape[0]) * np.concatenate((np.zeros([Theta_1.shape[0], 1]), Theta_1[:, 1:]), axis = 1)

Theta_2_derivative = (1/(X_1.shape[0]))*DELTA_2 + (lambda_reg/X_1.shape[0]) * np.concatenate((np.zeros([Theta_2.shape[0], 1]), Theta_2[:, 1:]), axis = 1)

alpha = 0.05

Theta_1_temp = alpha*Theta_1_derivative
Theta_2_temp = alpha*Theta_2_derivative

Theta_1 = Theta_1 - Theta_1_temp
Theta_2 = Theta_2 - Theta_2_temp



Theta_1.shape, Theta_2.shape

X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, random_state = 1)
X_1 = X_train
y = y_train

X0 = np.ones([X_1.shape[0],1])
#DO NOT TOUCH HERE (SO THE RAND GENERATED NUMBERS DO NOT CHANGE)
epsilon_init_1 = np.sqrt(6)/np.sqrt(hidden_layer + input_layer)
epsilon_init_2 = np.sqrt(6)/np.sqrt(hidden_layer + num_labels)

Theta_1 = np.random.rand(hidden_layer, input_layer+1)*epsilon_init_1*2 - epsilon_init_1
Theta_2 = np.random.rand(num_labels, hidden_layer+1)*epsilon_init_2*2 - epsilon_init_2
Theta_1_derivative = np.zeros(Theta_1.shape)
Theta_2_derivative = np.zeros(Theta_2.shape)
J = 10

count = 0

while J > 0.7:
  count = count+1
  #FEEDFORWARD
  X_1_X0 = np.concatenate((X0, X_1), axis=1) 
  
  Z2 = np.dot(X_1_X0,Theta_1.transpose())
  X_2 = 1/(1+(np.e**(-Z2)))
  
  X_2_X0 = np.concatenate((X0, X_2), axis=1) 
  
  Z3 = np.dot(X_2_X0, Theta_2.transpose())
  
  y_predict = 1/(1+(np.e**(-Z3)))
  
  # Regularization term
  lambda_reg = 1
  reg = (lambda_reg/(2*X_1.shape[0]))*(np.sum(np.sum((Theta_1[:,1:])**2, axis=1)) + np.sum(np.sum((Theta_2[:,1:])**2, axis=1)))
  
  Cost_func = np.sum(-y*np.log(y_predict) - (1-y)*np.log(1 - y_predict), axis=1)
  
  J = (1/Cost_func.size)*np.sum(Cost_func) + reg
  
  DELTA_1 = 0
  DELTA_2 = 0
  
  delta_3 = y_predict - y
  
  delta_2 = np.dot(delta_3, Theta_2)*np.concatenate((X0, X_2*(1-X_2)), axis=1)
  delta_2 = delta_2[:, 1:]
  
  DELTA_2 = np.dot(delta_3.transpose(), X_2_X0)
  DELTA_1 = np.dot(delta_2.transpose(), X_1_X0)
  
  Theta_1_derivative = (1/(X_1.shape[0]))*DELTA_1 + (lambda_reg/X_1.shape[0]) * np.concatenate((np.zeros([Theta_1.shape[0], 1]), Theta_1[:, 1:]), axis = 1)

  Theta_2_derivative = (1/(X_1.shape[0]))*DELTA_2 + (lambda_reg/X_1.shape[0]) * np.concatenate((np.zeros([Theta_2.shape[0], 1]), Theta_2[:, 1:]), axis = 1)
  alpha = 0.5

  Theta_1_temp = alpha*Theta_1_derivative
  Theta_2_temp = alpha*Theta_2_derivative

  Theta_1 = Theta_1 - Theta_1_temp
  Theta_2 = Theta_2 - Theta_2_temp

  if count%50 == 0:
    print(J)

print(f'Final Cost Value: {J}')

def predictions(Theta1, Theta2, Input_data, Real_Value):
  X0 = np.ones([Input_data.shape[0],1])
  Input_data_X0 = np.concatenate((X0, Input_data), axis=1) 
  
  Z2 = np.dot(Input_data_X0,Theta1.transpose())
  Hidden_layer = 1/(1+(np.e**(-Z2)))
  
  Hidden_layer_X0 = np.concatenate((X0, Hidden_layer), axis=1) 
  
  Z3 = np.dot(Hidden_layer_X0, Theta2.transpose())
  
  prediction = 1/(1+(np.e**(-Z3)))
  predict_1 = np.copy(prediction)

  for i in range(prediction.shape[0]):
    i_max = prediction[i].argmax()
    for j in range(prediction[i].shape[0]):
      if j == i_max:
        prediction[i][j] = 1
      else:
        prediction[i][j] = 0
  #comparative = np.concatenate((prediction, Real_Value), axis=1)
  return prediction, predict_1

matrix, matrix1 = predictions(Theta_1, Theta_2, X_test, y_test)

Correct = 0
Wrong = 0
for i in range(y_test.shape[0]):
  if (y_test[i] == matrix[i]).all():
    Correct = Correct+1
  else:
    Wrong = Wrong+1
Accuracy = (Correct/(y_test.shape[0]))*100
Correct, Wrong, Accuracy

"""## Using sklearn MLPClassifier"""

from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(hidden_layer,),solver= 'lbfgs', activation='logistic', max_iter=2000, learning_rate= 'invscaling', learning_rate_init=0.5, alpha=1, random_state=1)

model.fit(X_1, y)

y_predict =  model.predict(X_test)
Correct = 0
Wrong = 0
for i in range(y_test.shape[0]):
  if (y_test[i] == y_predict[i]).all():
    Correct = Correct+1
  else:
    Wrong = Wrong+1
Accuracy = (Correct/(y_test.shape[0]))*100
Correct, Wrong, Accuracy

